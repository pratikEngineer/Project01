package sparkfunctionalities

import org.apache.spark.{SparkConf, SparkContext}

  object sortbykey {

    def main(args: Array[String]): Unit = {

      // 1Ô∏è‚É£ Create Spark configuration
      val conf = new SparkConf()
        .setAppName("IPAnalysis")
        .setMaster("local[*]")

      // 2Ô∏è‚É£ Create SparkContext
      val sc = new SparkContext(conf)

      // 3Ô∏è‚É£ Input data
      val data = List(
        "168.182.0.1.1","168.182.0.1.2","168.182.0.1.1","168.182.0.1.3",
        "168.182.0.1.2","168.182.0.1.1","168.182.0.1.3","168.182.0.1.4",
        "168.182.0.1.3","168.182.0.1.1","168.182.0.1.3","168.182.0.1.2",
        "168.182.0.1.4","168.182.0.1.1","168.182.0.1.2","168.182.0.1.1"
      )

      // 4Ô∏è‚É£ Create RDD
      val rdd = sc.parallelize(data)

      // 5Ô∏è‚É£ Convert to (ip,1)
      val pairRDD = rdd.map(x => (x, 1))

      // 6Ô∏è‚É£ reduceByKey
      val countRDD = pairRDD.reduceByKey(_ + _)

      // 7Ô∏è‚É£ sortByKey
      val sortByKeyRDD = countRDD.sortByKey()

      // 8Ô∏è‚É£ sortBy count descending
      val sortByCountRDD = countRDD.sortBy(_._2, ascending = false)

      // 9Ô∏è‚É£ groupByKey
      val groupByKeyRDD = pairRDD.groupByKey().mapValues(_.sum)

      // üîü Second highest IP
      val secondHighest = sortByCountRDD.take(2)(1)

      // üîç Output
      println("Counts: " + countRDD.collect().toList)
      println("Sorted by count: " + sortByCountRDD.collect().toList)
      println("Second highest IP: " + secondHighest)

      // 1Ô∏è‚É£1Ô∏è‚É£ Stop Spark
      sc.stop()
    }


}
